{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ee828c-079a-4afe-8ada-b3a2569bb284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f4aba6-46ca-4ec0-afea-a62c87da8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()\n",
    "browser.get(\"https://www.amazon.in/gp/bestsellers/?ref_=nav_em_cs_bestsellers_0_1_1_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83adc0f2-f50b-4d2d-a1c4-88960f32f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = browser.page_source\n",
    "doc = BeautifulSoup(html_content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aeb61fa4-ce37-45f9-8937-cf2ce8d494b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Department 1 Amazon Launchpad\n",
      "Scraping Category 1 Body\n",
      "Scraping Sub Category 1 Beauty\n",
      "Scraping Product 1\n",
      "Scraping Product 2\n",
      "Scraping Product 3\n",
      "Scraping Product 4\n",
      "Scraping Product 5\n",
      "Scraping Product 6\n",
      "Scraping Product 7\n",
      "Scraping Product 8\n",
      "Scraping Product 9\n",
      "Scraping Product 10\n",
      "Scraping Product 11\n",
      "Scraping Product 12\n",
      "Scraping Product 13\n",
      "Scraping Product 14\n",
      "Scraping Product 15\n",
      "Scraping Product 16\n",
      "Scraping Product 17\n",
      "Scraping Product 18\n",
      "Scraping Product 19\n",
      "Scraping Product 20\n",
      "Scraping Product 21\n",
      "Scraping Product 22\n",
      "Scraping Product 23\n",
      "Scraping Product 24\n",
      "Scraping Product 25\n",
      "Scraping Product 26\n",
      "Scraping Product 27\n",
      "Scraping Product 28\n",
      "Scraping Product 29\n",
      "Scraping Product 30\n"
     ]
    }
   ],
   "source": [
    "browser = webdriver.Edge()\n",
    "browser.maximize_window()\n",
    "browser.get(\"https://www.amazon.in/gp/bestsellers/?ref_=nav_em_cs_bestsellers_0_1_1_2\")\n",
    "\n",
    "html_content = browser.page_source\n",
    "doc = BeautifulSoup(html_content, \"html.parser\")\n",
    "data = []\n",
    "\n",
    "# Getting all Departments\n",
    "department_names = []\n",
    "department_urls = []\n",
    "base = \"https://www.amazon.in\"\n",
    "department_tags = doc.find_all(\"div\", {\"role\": \"group\"})[0].find_all(\"a\")\n",
    "for i in range(0, len(department_tags)):\n",
    "    department_names.append(department_tags[i].text.strip())\n",
    "    department_urls.append(base + department_tags[i][\"href\"])\n",
    "\n",
    "# Getting Categories\n",
    "for i in range(0, 1):\n",
    "    print(f\"Scraping Department {i + 1} {department_names[i]}\")\n",
    "    browser.get(department_urls[i])\n",
    "    html_content = browser.page_source\n",
    "    doc = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    category_tags = doc.find_all(\"div\", {\"role\": \"group\"})[0].find_all(\"a\")\n",
    "    special = base + category_tags[0][\"href\"]\n",
    "    for j in range(0, 1):\n",
    "        print(f\"Scraping Category {j + 1} {category_tags[j].text.strip()}\")\n",
    "        category_name = category_tags[j].text.strip()\n",
    "        category_url = base + category_tags[j][\"href\"]\n",
    "\n",
    "        # Getting Sub Categories\n",
    "        browser.get(category_url)\n",
    "        html_content = browser.page_source\n",
    "        doc = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        sub_category_tags = doc.find_all(\"div\", {\"role\": \"group\"})[0].find_all(\"a\")\n",
    "        for k in range(0, 1):\n",
    "            if special in base + sub_category_tags[0][\"href\"]:\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Scraping Sub Category {k + 1} {sub_category_tags[k].text.strip()}\")\n",
    "                sub_category_name = sub_category_tags[k].text.strip()\n",
    "                sub_category_url = base + sub_category_tags[k][\"href\"]\n",
    "\n",
    "                # Getting Product Links\n",
    "                browser.get(sub_category_url)\n",
    "                product_links = []\n",
    "                \n",
    "                for l in range(0, 2):\n",
    "                    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(2)\n",
    "                    html_content = browser.page_source\n",
    "                    doc = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "                    product_tags = doc.find_all(\"div\", {\"id\": \"gridItemRoot\"})\n",
    "                    # print(len(product_tags))\n",
    "                    for m in range(0, len(product_tags)):\n",
    "                        product_links.append(base + product_tags[m].find(\"a\")[\"href\"])\n",
    "    \n",
    "                    # Going to next page\n",
    "                    if l == 1:\n",
    "                        break\n",
    "                    try:\n",
    "                        next_page_tag = doc.find_all(\"div\", {\"class\": \"a-text-center\"})[-1].find(\"li\", {\"class\": \"a-last\"}).find(\"a\")\n",
    "                        browser.get(base + next_page_tag[\"href\"])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                # Getting Info of each product\n",
    "                for l in range(0, 30):\n",
    "                    print(f\"Scraping Product {l + 1}\")\n",
    "                    browser.get(product_links[l])\n",
    "                    html_content = browser.page_source\n",
    "                    doc = BeautifulSoup(html_content, \"html.parser\")\n",
    "                    product_data = []\n",
    "\n",
    "                    product_name = doc.find(\"span\", {\"id\": \"productTitle\"}).text.strip()\n",
    "                    try:\n",
    "                        price_div = doc.find(\"div\", {\"class\": \"a-section a-spacing-none aok-align-center aok-relative\"})\n",
    "                        product_price = price_div.find(\"span\", {\"class\": \"a-price-whole\"}).text.strip()\n",
    "                        sale_discount = price_div.find(\"span\", {\"class\": \"a-size-large a-color-price savingPriceOverride aok-align-center reinventPriceSavingsPercentageMargin savingsPercentage\"}).text.strip().split(\"-\")[1]\n",
    "                    except:\n",
    "                        product_price = None\n",
    "                        sale_discount = None\n",
    "                        \n",
    "                    best_seller_rating = doc.find(\"div\", {\"class\": \"a-section a-spacing-base brand-snapshot-flex-row\"}).text.strip().split(\"%\")[0] + \"%\"\n",
    "                    try:\n",
    "                        ship_from = doc.find(\"div\", {\"id\": \"tabular-buybox\"}).find_all(\"div\", {\"tabular-attribute-name\": \"Ships from\"})[1].text.strip()\n",
    "                    except:\n",
    "                        ship_from = None\n",
    "                    \n",
    "                    sold_by = doc.find(\"div\", {\"class\": \"a-section a-spacing-medium brand-snapshot-flex-row\"}).find(\"span\").text.strip()\n",
    "                    rating = doc.find(\"span\", {\"id\": \"acrPopover\"}).find(\"a\").find(\"span\").text.strip()\n",
    "                    product_description = doc.find_all(\"div\", {\"id\": \"aplus\"})[-1].text.strip().replace(\"Product Description\", \"\").replace(\"\\n\", \"\").replace(\"  \", \" \")\n",
    "                    try:\n",
    "                        number_of_bought = doc.find(\"span\", {\"id\": \"social-proofing-faceout-title-tk_bought\"}).find(\"span\").text.strip().split(\"+\")[0].replace(\"K\", \"000\")\n",
    "                    except:\n",
    "                        number_of_bought = None\n",
    "\n",
    "                    data.append([product_name, product_price, sale_discount, best_seller_rating, ship_from, sold_by, rating, product_description, number_of_bought, department_names[i]])\n",
    "\n",
    "# print(data)\n",
    "df = pd.DataFrame(data, columns=[\"Product Name\", \"Product Price\", \"Sale Discount\", \"Best Seller Rating\", \"Ship From\", \"Sold by\", \"Rating\", \"Product Description\", \"Number of Bought\", \"Category\"])\n",
    "df = df.sort_values(by=\"Sale Discount\", ascending=False)\n",
    "df.to_csv(\"data.csv\", index=None)\n",
    "\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff894e0-5b59-4725-81c2-32602c80b114",
   "metadata": {},
   "source": [
    "<h2>Info of each product</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20d164da-1917-45bb-9e72-0ad66b134500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87% Amazon\n",
      "COS-IQ\n",
      "4.0\n",
      "\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "browser = webdriver.Edge()\n",
    "browser.get(\"https://www.amazon.in/Niacinamide-Multi-Peptide-Hyaluronic-Correction-Hyperpigmentation/dp/B097KSJ6LR/ref=zg_bs_g_10894225031_d_sccl_2/262-3973435-7950060?th=1\")\n",
    "html_content = browser.page_source\n",
    "doc = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "best_seller_rating = doc.find(\"div\", {\"class\": \"a-section a-spacing-base brand-snapshot-flex-row\"}).text.strip().split(\"%\")[0] + \"%\"\n",
    "\n",
    "try:\n",
    "    ships_tag = doc.find(\"div\", {\"id\": \"tabular-buybox\"}).find_all(\"div\", {\"tabular-attribute-name\": \"Ships from\"})[1].text.strip()\n",
    "except:\n",
    "    ships_tag = None\n",
    "print(best_seller_rating, ships_tag)\n",
    "\n",
    "sold_by = doc.find(\"div\", {\"class\": \"a-section a-spacing-medium brand-snapshot-flex-row\"}).find(\"span\").text.strip()\n",
    "print(sold_by)\n",
    "\n",
    "rating = doc.find(\"span\", {\"id\": \"acrPopover\"}).find(\"a\").find(\"span\").text.strip()\n",
    "print(rating)\n",
    "\n",
    "product_description = doc.find_all(\"div\", {\"id\": \"aplus\"})[1].text.strip().replace(\"Product Description\", \"\")\n",
    "print(product_description)\n",
    "\n",
    "number_of_bought = doc.find(\"span\", {\"id\": \"social-proofing-faceout-title-tk_bought\"}).find(\"span\").text.strip().split(\"+\")[0].replace(\"K\", \"000\")\n",
    "print(number_of_bought)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
